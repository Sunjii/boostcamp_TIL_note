# BERT 학습

## BERT 학습 단계

1. Tokenizer 만들기
2. 데이터셋 확보
3. Next Sentence Prediction (NSP)
4. Masking

BERT의 경우 대용량의 corpus를 필요로한다. 모델 사이즈가 너무 크기 때문이다. 마지막으로 BERT는 2가지 요소로 학습을 한다. NSP와 Masking이다.

## 왜 학습을 다시?

이미 있는 거 쓰지, 왜 새로 학습을 할까요? 도메인 특화 task의 경우, 도메인 특화된 학습 데이터만 사용하는 것이 성능이 더 좋다.

실험적으로 도메인 특화 task의 경우 fine-tuning을 하는 것 보다는 도메인 corpus로 다시 pre-train을 시키는 것이 더 성능이 좋음을 보임.

## 학습을 위한 데이터 만들기

그렇다면 우리의 목표는 원하는 데이터 셋을 만드는 것이다. 데이터를 모델에 입력 가능한 꼴로 만드는 것이다. BERT의 경우 토큰 단위로 입력을 받으며, 토큰의 타입과 postion 정보 등이 들어간다. 즉, 데이터를 위와 같은 형식으로 변환하여 데이터 셋으로 만든다. 다음으로는 Masking 작업이 들어가야 한다.

## Mask attack?

학습을 위한 데이터셋 구성시 개인정보를 모두 빼야하는 이유! [Mask] 처리를 하여 원하는 정보를 뽑아낼 수 있기 때문.
